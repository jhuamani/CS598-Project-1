# Check if house has been remodeled
data$Remodeled = 0
data$Remodeled[which((data$Year_Remod_Add - data$Year_Built) > 0)] = 1
data = subset(data, select=-c(Year_Remod_Add))
# Age of purchase
data$House_Age = data$Year_Sold - data$Year_Built
data = subset(data, select=-c(Year_Sold, Year_Built))
# Total house square footage, excluded Mas_Vnr_Area, Lot_Area and Pool_Area
data$Total_SF = data$Total_Bsmt_SF + data$First_Flr_SF + data$Second_Flr_SF + data$Low_Qual_Fin_SF + data$Gr_Liv_Area + data$Garage_Area + data$Wood_Deck_SF + data$Open_Porch_SF + data$Enclosed_Porch + data$Three_season_porch + data$Screen_Porch
data = subset(data, select=-c(Total_Bsmt_SF, BsmtFin_SF_1, BsmtFin_SF_2, Bsmt_Unf_SF, First_Flr_SF, Second_Flr_SF, Low_Qual_Fin_SF, Gr_Liv_Area, Garage_Area, Wood_Deck_SF, Open_Porch_SF, Enclosed_Porch, Three_season_porch, Screen_Porch))
# Hot encode the data
data_mod = dummyVars(~ .,data=data, fullRank = forLinear)
data = data.frame(predict(data_mod, newdata = data))
data
}
# Cleaned and encoded trainData
trainData = clean_data(trainData, forLinear = FALSE)
# Cleaned and encoded testData
testData = clean_data(testData, forLinear = FALSE)
print(dim(trainData))
print(dim(testData))
names_intest_intrain = names(testData)[which((names(testData) %in% names(trainData)))]
names_intrain_nottest = names(trainData)[which(!(names(trainData) %in% names(testData)))]
# Remove any predictors in testData not present in trainData
if (length(names_intest_intrain) != 0) {
testData = testData[,names_intest_intrain]
}
# For any predictor present in trainData but not testData add 0 columns
if (length(names_intrain_nottest) != 0) {
for (name in names_intrain_nottest) {
if (name != "Sale_Price"){
testData[,name] = 0
}
}
}
# Final check to ensure that only column that is different is Sale_Price
print(names(testData)[which(!(names(testData) %in% names(trainData)))])
print(names(trainData)[which(!(names(trainData) %in% names(testData)))])
train_mat_df = subset(trainData, select=-c(Sale_Price))
train_mat = as.matrix(train_mat_df[,order(colnames(train_mat_df))])
train_Y = trainData$Sale_Price
test_mat = as.matrix(testData[,order(colnames(testData))])
print(dim(train_mat))
print(dim(test_mat))
# Original eta = 0.05 and nrounds = 5000
xgb.model = xgboost(data = train_mat,
label = train_Y, max_depth = 6,
eta = 0.04, nrounds = 6000,
subsample = 0.5,
verbose = FALSE)
summary(xgb.model)
tmp = predict(xgb.model, test_mat)
sqrt(mean((log(tmp) - log(test.y[,2]))^2))
stop.time = Sys.time()
print(start.time - stop.time)
start.time = Sys.time()
library(caret)
library(xgboost)
set.seed(2064)
myData = read.csv("Ames_data.csv")
testIDs = read.table("project1_testIDs.dat")
j = 4
train = myData[-testIDs[,j], ]
test = myData[testIDs[,j], ]
test.y = test[, c(1, 83)]
test = test[, -83]
write.csv(train,"train.csv",row.names=FALSE)
write.csv(test, "test.csv",row.names=FALSE)
write.csv(test.y,"test_y.csv",row.names=FALSE)
trainData = read.csv("train.csv")
testData = read.csv("test.csv")
test.y = read.csv("test_y.csv")
i = 0
start = 0
stop = 50
for (name in names(trainData[,sapply(trainData,class) == "character"])) {
if (i >= start){
cat("\nPredictor ",name,"\n\n")
print(unique(trainData[,name]))
}
i = i+1
if (i == stop) {break}
}
# Here we check if any missing values exist in any predictor
NAcols = apply(trainData,2, function(x) any(is.na(x)))
print(which(NAcols==TRUE))
clean_data = function(data, forLinear = TRUE){
# Electrical Column: Replace Unknown with highest frequency SBrkr
data[which(data$Electrical == "Unknown"),"Electrical"] = "SBrkr"
# Outlier in Garage_Yr_Blt
data[which(data$Garage_Yr_Blt == 2207),"Garage_Yr_Blt"] = 2006
data[,"Garage_Yr_Blt"] = as.character(data[,"Garage_Yr_Blt"])
data[is.na(data[,"Garage_Yr_Blt"]),"Garage_Yr_Blt"] = "No_Garage"
# After revieweing correlations, Garage_Yr_Blt should be removed as it adds too many excess columns with not much added value
data = subset(data, select=-c(Garage_Yr_Blt))
# Outlier constraint to 95% quantile
num.vars = c("Lot_Frontage", "Lot_Area", "Mas_Vnr_Area", "BsmtFin_SF_2", "Bsmt_Unf_SF", "Total_Bsmt_SF", "Second_Flr_SF", 'First_Flr_SF', "Gr_Liv_Area", "Garage_Area", "Wood_Deck_SF", "Open_Porch_SF", "Enclosed_Porch", "Three_season_porch", "Screen_Porch", "Misc_Val")
quan.val = 0.95
for (var in num.vars) {
temp = data[,var]
quan = quantile(temp, probs = quan.val, na.rm = TRUE)
temp[temp > quan] = quan
data[,var] = temp
}
# Remove unnecessary columns
data = subset(data, select=-c(Street, Utilities, Condition_2, Roof_Matl, Heating, Pool_QC, Misc_Feature, Pool_Area, Longitude,Latitude))
# Outputs are not the same, some houses with No_Garages have a Garage_Type filled in and Garage_Cars > 0. This needs to be corrected
# A single entry has a Garage_Area of 360, we'll assume that it's 0
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Cars"] = 0
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Type"] = "No_Garage"
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Area"] = 0
# Combine bathrooms drop other bathrooms
data$Total_Baths = data$Bsmt_Full_Bath + data$Full_Bath + 0.5 * (data$Bsmt_Half_Bath + data$Half_Bath)
data = subset(data, select=-c(Bsmt_Full_Bath, Full_Bath, Bsmt_Half_Bath, Half_Bath))
# Since Garage Cars and Garage Area are highly correlated, drop Garage Cars
data = subset(data, select=-c(Garage_Cars))
# Check if house has been remodeled
data$Remodeled = 0
data$Remodeled[which((data$Year_Remod_Add - data$Year_Built) > 0)] = 1
data = subset(data, select=-c(Year_Remod_Add))
# Age of purchase
data$House_Age = data$Year_Sold - data$Year_Built
data = subset(data, select=-c(Year_Sold, Year_Built))
# Total house square footage, excluded Mas_Vnr_Area, Lot_Area and Pool_Area
data$Total_SF = data$Total_Bsmt_SF + data$First_Flr_SF + data$Second_Flr_SF + data$Low_Qual_Fin_SF + data$Gr_Liv_Area + data$Garage_Area + data$Wood_Deck_SF + data$Open_Porch_SF + data$Enclosed_Porch + data$Three_season_porch + data$Screen_Porch
data = subset(data, select=-c(Total_Bsmt_SF, BsmtFin_SF_1, BsmtFin_SF_2, Bsmt_Unf_SF, First_Flr_SF, Second_Flr_SF, Low_Qual_Fin_SF, Gr_Liv_Area, Garage_Area, Wood_Deck_SF, Open_Porch_SF, Enclosed_Porch, Three_season_porch, Screen_Porch))
# Hot encode the data
data_mod = dummyVars(~ .,data=data, fullRank = forLinear)
data = data.frame(predict(data_mod, newdata = data))
data
}
# Cleaned and encoded trainData
trainData = clean_data(trainData, forLinear = FALSE)
# Cleaned and encoded testData
testData = clean_data(testData, forLinear = FALSE)
print(dim(trainData))
print(dim(testData))
names_intest_intrain = names(testData)[which((names(testData) %in% names(trainData)))]
names_intrain_nottest = names(trainData)[which(!(names(trainData) %in% names(testData)))]
# Remove any predictors in testData not present in trainData
if (length(names_intest_intrain) != 0) {
testData = testData[,names_intest_intrain]
}
# For any predictor present in trainData but not testData add 0 columns
if (length(names_intrain_nottest) != 0) {
for (name in names_intrain_nottest) {
if (name != "Sale_Price"){
testData[,name] = 0
}
}
}
# Final check to ensure that only column that is different is Sale_Price
print(names(testData)[which(!(names(testData) %in% names(trainData)))])
print(names(trainData)[which(!(names(trainData) %in% names(testData)))])
train_mat_df = subset(trainData, select=-c(Sale_Price))
train_mat = as.matrix(train_mat_df[,order(colnames(train_mat_df))])
train_Y = trainData$Sale_Price
test_mat = as.matrix(testData[,order(colnames(testData))])
print(dim(train_mat))
print(dim(test_mat))
# Original eta = 0.05 and nrounds = 5000
xgb.model = xgboost(data = train_mat,
label = train_Y, max_depth = 6,
eta = 0.04, nrounds = 6000,
subsample = 0.5,
verbose = FALSE)
summary(xgb.model)
tmp = predict(xgb.model, test_mat)
sqrt(mean((log(tmp) - log(test.y[,2]))^2))
stop.time = Sys.time()
print(start.time - stop.time)
start.time = Sys.time()
library(caret)
library(xgboost)
set.seed(2064)
start.time = Sys.time()
library(caret)
library(xgboost)
set.seed(2064)
myData = read.csv("Ames_data.csv")
testIDs = read.table("project1_testIDs.dat")
j = 5
train = myData[-testIDs[,j], ]
test = myData[testIDs[,j], ]
test.y = test[, c(1, 83)]
test = test[, -83]
write.csv(train,"train.csv",row.names=FALSE)
write.csv(test, "test.csv",row.names=FALSE)
write.csv(test.y,"test_y.csv",row.names=FALSE)
trainData = read.csv("train.csv")
testData = read.csv("test.csv")
test.y = read.csv("test_y.csv")
i = 0
start = 0
stop = 50
for (name in names(trainData[,sapply(trainData,class) == "character"])) {
if (i >= start){
cat("\nPredictor ",name,"\n\n")
print(unique(trainData[,name]))
}
i = i+1
if (i == stop) {break}
}
# Here we check if any missing values exist in any predictor
NAcols = apply(trainData,2, function(x) any(is.na(x)))
print(which(NAcols==TRUE))
clean_data = function(data, forLinear = TRUE){
# Electrical Column: Replace Unknown with highest frequency SBrkr
data[which(data$Electrical == "Unknown"),"Electrical"] = "SBrkr"
# Outlier in Garage_Yr_Blt
data[which(data$Garage_Yr_Blt == 2207),"Garage_Yr_Blt"] = 2006
data[,"Garage_Yr_Blt"] = as.character(data[,"Garage_Yr_Blt"])
data[is.na(data[,"Garage_Yr_Blt"]),"Garage_Yr_Blt"] = "No_Garage"
# After revieweing correlations, Garage_Yr_Blt should be removed as it adds too many excess columns with not much added value
data = subset(data, select=-c(Garage_Yr_Blt))
# Outlier constraint to 95% quantile
num.vars = c("Lot_Frontage", "Lot_Area", "Mas_Vnr_Area", "BsmtFin_SF_2", "Bsmt_Unf_SF", "Total_Bsmt_SF", "Second_Flr_SF", 'First_Flr_SF', "Gr_Liv_Area", "Garage_Area", "Wood_Deck_SF", "Open_Porch_SF", "Enclosed_Porch", "Three_season_porch", "Screen_Porch", "Misc_Val")
quan.val = 0.95
for (var in num.vars) {
temp = data[,var]
quan = quantile(temp, probs = quan.val, na.rm = TRUE)
temp[temp > quan] = quan
data[,var] = temp
}
# Remove unnecessary columns
data = subset(data, select=-c(Street, Utilities, Condition_2, Roof_Matl, Heating, Pool_QC, Misc_Feature, Pool_Area, Longitude,Latitude))
# Outputs are not the same, some houses with No_Garages have a Garage_Type filled in and Garage_Cars > 0. This needs to be corrected
# A single entry has a Garage_Area of 360, we'll assume that it's 0
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Cars"] = 0
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Type"] = "No_Garage"
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Area"] = 0
# Combine bathrooms drop other bathrooms
data$Total_Baths = data$Bsmt_Full_Bath + data$Full_Bath + 0.5 * (data$Bsmt_Half_Bath + data$Half_Bath)
data = subset(data, select=-c(Bsmt_Full_Bath, Full_Bath, Bsmt_Half_Bath, Half_Bath))
# Since Garage Cars and Garage Area are highly correlated, drop Garage Cars
data = subset(data, select=-c(Garage_Cars))
# Check if house has been remodeled
data$Remodeled = 0
data$Remodeled[which((data$Year_Remod_Add - data$Year_Built) > 0)] = 1
data = subset(data, select=-c(Year_Remod_Add))
# Age of purchase
data$House_Age = data$Year_Sold - data$Year_Built
data = subset(data, select=-c(Year_Sold, Year_Built))
# Total house square footage, excluded Mas_Vnr_Area, Lot_Area and Pool_Area
data$Total_SF = data$Total_Bsmt_SF + data$First_Flr_SF + data$Second_Flr_SF + data$Low_Qual_Fin_SF + data$Gr_Liv_Area + data$Garage_Area + data$Wood_Deck_SF + data$Open_Porch_SF + data$Enclosed_Porch + data$Three_season_porch + data$Screen_Porch
data = subset(data, select=-c(Total_Bsmt_SF, BsmtFin_SF_1, BsmtFin_SF_2, Bsmt_Unf_SF, First_Flr_SF, Second_Flr_SF, Low_Qual_Fin_SF, Gr_Liv_Area, Garage_Area, Wood_Deck_SF, Open_Porch_SF, Enclosed_Porch, Three_season_porch, Screen_Porch))
# Hot encode the data
data_mod = dummyVars(~ .,data=data, fullRank = forLinear)
data = data.frame(predict(data_mod, newdata = data))
data
}
# Cleaned and encoded trainData
trainData = clean_data(trainData, forLinear = FALSE)
# Cleaned and encoded testData
testData = clean_data(testData, forLinear = FALSE)
print(dim(trainData))
print(dim(testData))
names_intest_intrain = names(testData)[which((names(testData) %in% names(trainData)))]
names_intrain_nottest = names(trainData)[which(!(names(trainData) %in% names(testData)))]
# Remove any predictors in testData not present in trainData
if (length(names_intest_intrain) != 0) {
testData = testData[,names_intest_intrain]
}
# For any predictor present in trainData but not testData add 0 columns
if (length(names_intrain_nottest) != 0) {
for (name in names_intrain_nottest) {
if (name != "Sale_Price"){
testData[,name] = 0
}
}
}
# Final check to ensure that only column that is different is Sale_Price
print(names(testData)[which(!(names(testData) %in% names(trainData)))])
print(names(trainData)[which(!(names(trainData) %in% names(testData)))])
train_mat_df = subset(trainData, select=-c(Sale_Price))
train_mat = as.matrix(train_mat_df[,order(colnames(train_mat_df))])
train_Y = trainData$Sale_Price
test_mat = as.matrix(testData[,order(colnames(testData))])
print(dim(train_mat))
print(dim(test_mat))
# Original eta = 0.05 and nrounds = 5000
xgb.model = xgboost(data = train_mat,
label = train_Y, max_depth = 6,
eta = 0.04, nrounds = 6000,
subsample = 0.5,
verbose = FALSE)
summary(xgb.model)
tmp = predict(xgb.model, test_mat)
sqrt(mean((log(tmp) - log(test.y[,2]))^2))
stop.time = Sys.time()
print(start.time - stop.time)
library(MASS)
attach(Boston)
library(MASS)
attach(Boston)
dis
q8 = poly(nox ~ dis, degree = 3)
head(Boston)
#q8 = lm(nox ~ poly(dis,3), )
#head(Boston)
q8 = lm(nox ~ poly(dis,3), Boston)
#head(Boston)
q8 = lm(nox ~ poly(dis,3), Boston)
summary(q8)
q8$residuals
sum((q8$residuals)^2)
predict(q8, newdata=list(dis=6))
q11 = lm(nox ~ poly(dis,4), Boston)
summary(q11)
sum((q11$residuals)^2)
predict(q11, newdata=list(dis=6))
myfit1 = lm(nox ~ bs(dis, df=3), data=Boston)
library(splines)
myfit1 = lm(nox ~ bs(dis, df=3), data=Boston)
myfit2 = lm(nox ~ bs(dis, df=4, intercept=TRUE), data=Boston)
myfit3 = lm(nox ~ poly(dis,3), data=Boston)
print(sum((myfit1$residuals)^2))
print(sum((myfit2$residuals)^2))
print(sum((myfit3$residuals)^2))
library(splines)
myfit1 = lm(nox ~ bs(dis, df=4), data=Boston)
myfit2 = lm(nox ~ bs(dis, df=5, intercept=TRUE), data=Boston)
myfit3 = lm(nox ~ bs(dis,knots=median(dis)), data=Boston)
print(sum((myfit1$residuals)^2))
print(sum((myfit2$residuals)^2))
print(sum((myfit3$residuals)^2))
Estep <- function(data, G, para){
# Your Code
# Return the n-by-G probability matrix
n = dim(data)[1]
z = outer(1:n, 1:G,FUN = Vectorize(function(i, j)
-(1/2) * as.matrix(data[i,] - para$mean[,j]) %*% solve(para$Sigma) %*% t(as.matrix(data[i,] - para$mean[,j])) + log(para$prob[j]) )
)
# Subtracting max value of z to handle underflow problem
z_max = apply(z, 1, max)
z = exp(z - z_max) / rowSums(exp(z - z_max))
z
}
Mstep <- function(data, G, para, post.prob){
# Your Code
# Return the updated parameters
n = dim(data)[1]
d = dim(data)[2]
para$prob = colSums(post.prob) / n
para$mean = outer(1:d, 1:G,FUN = Vectorize(function(i, j)
sum(as.matrix(post.prob[,j]*data[,i])) / sum(as.matrix(post.prob[,j]))
) )
para$Sigma = matrix(0,d,d)
for (g in 1:G){
for (i in 1:n){
para$Sigma = para$Sigma + (t(data[i,]) - para$mean[,g]) %*% t(t(data[i,]) - para$mean[,g]) * post.prob[i,g]
}
}
para$Sigma = para$Sigma / n
para
}
myEM <- function(data, itmax, G, para){
# itmax: num of iterations
# G:     num of components
# para:  list of parameters (prob, mean, Sigma)
for(t in 1:itmax){
post.prob <- Estep(data, G, para)
para <- Mstep(data, G, para, post.prob)
}
return(para)
}
options(digits=8)
options()$digits
library(mclust)
dim(faithful)
head(faithful)
n = nrow(faithful)
K <- 2
set.seed(2064)
gID <- sample(1:K, n, replace = TRUE)
Z <- matrix(0, n, K)
for(k in 1:K)
Z[gID == k, k] <- 1
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
para0 <- list(prob = ini0$pro,
mean = ini0$mean,
Sigma = ini0$variance$Sigma)
para0
myEM(data=faithful, itmax=20, G=K, para=para0)
Rout <- em(modelName = "EEE", data = faithful,
control = emControl(eps=0, tol=0, itmax = 20),
parameters = ini0)$parameters
list(Rout$pro, Rout$mean, Rout$variance$Sigma)
K <- 3
gID <- sample(1:K, n, replace = TRUE)
Z <- matrix(0, n, K)
for(k in 1:K)
Z[gID == k, k] <- 1
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
para0 <- list(prob = ini0$pro,
mean = ini0$mean,
Sigma = ini0$variance$Sigma)
para0
myEM(data=faithful, itmax=20, G=K, para=para0)
Rout <- em(modelName = "EEE", data = faithful,
control = emControl(eps=0, tol=0, itmax = 20),
parameters = ini0)$parameters
list(Rout$pro, Rout$mean, Rout$variance$Sigma)
setwd("~/GitHub/CS598-Project-1/Project 2")
data = read.csv("train.csv")
data = read.csv("train.csv")
summary(data)
summary(data)
head(data)
summary(data)
head(data)
data$Weekly_Sales < 0
summary(data)
head(data)
data[data$Weekly_Sales < 0,]
summary(data)
head(data)
mypredict = function(){
start_date <- ymd("2011-03-01") %m+% months(2 * (t - 1))
end_date <- ymd("2011-05-01") %m+% months(2 * (t - 1))
test_current <- test %>%
filter(Date >= start_date & Date < end_date) %>%
select(-IsHoliday)
most_recent_date <- max(train$Date)
tmp_train <- train %>%
filter(Date == most_recent_date) %>%
rename(Weekly_Pred = Weekly_Sales) %>%
select(-Date, -IsHoliday)
test_pred <- test_current %>%
left_join(tmp_train, by = c('Dept', 'Store'))
return(test_pred)
}
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
library(lubridate)
install.packages("tidyverse")
library(lubridate)
library(tidyverse)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
# save weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
# load fold file
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file, col_types = cols())
train <- train %>% add_row(new_train)
# extract predictions matching up to the current fold
scoring_tbl <- new_train %>%
left_join(test_pred, by = c('Date', 'Store', 'Dept'))
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}
library(lubridate)
library(tidyverse)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# save weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
# load fold file
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file, col_types = cols())
train <- train %>% add_row(new_train)
# extract predictions matching up to the current fold
scoring_tbl <- new_train %>%
left_join(test_pred, by = c('Date', 'Store', 'Dept'))
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}
print(wae)
mean(wae)
# read in train / test dataframes
train <- read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# save weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
# load fold file
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file, col_types = cols())
train <- train %>% add_row(new_train)
# extract predictions matching up to the current fold
scoring_tbl <- new_train %>%
left_join(test_pred, by = c('Date', 'Store', 'Dept'))
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}
print(wae)
mean(wae)
