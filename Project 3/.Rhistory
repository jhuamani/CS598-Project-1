train = read.table(paste("split_1/train.tsv", sep=""),
stringsAsFactors = FALSE,
header = TRUE)
for (j in 2:5){
temp_train = read.table(paste("split_", j, "/train.tsv", sep=""),
stringsAsFactors = FALSE,
header = TRUE)
train = rbind(train, temp_train)
}
# Regex operation to match html tags in a non-greedy manner
train$review = gsub('<.*?>', ' ', train$review)
library(text2vec)
stop_words = c("i", "me", "my", "myself",
"we", "our", "ours", "ourselves",
"you", "your", "yours",
"their", "they", "his", "her",
"she", "he", "a", "an", "and",
"is", "was", "are", "were",
"him", "himself", "has", "have",
"it", "its", "the", "us")
# Creates an iterator, ensures all text is lowercase and uses a word tokenizer for a DTM
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
# Creates a vocabulary, excluding stopwords, of 1 to 4 grams
tmp.vocab = create_vocabulary(it_train,
stopwords = stop_words,
ngram = c(1L,4L))
# Reduce vocabulary size by accepting terms w/ min 10 count and that appear b/w 0.1% - 50% of docs
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(tmp.vocab)
# Create a DTM using the iterator and vocabulary
dtm_train  = create_dtm(it_train, vectorizer)
library(slam)
v.size = dim(dtm_train)[2]
ytrain = train$sentiment
summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
summ[,2] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)
summ[,3] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)
summ[,4] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)
n1 = sum(ytrain);
n = length(ytrain)
n0 = n - n1
# 2 sample t statistic
myp = (summ[,1] - summ[,3])/ sqrt(summ[,2]/n1 + summ[,4]/n0)
words = colnames(dtm_train)
id = order(abs(myp), decreasing=TRUE)[1:2000]
pos.list = words[id[myp[id]>0]]
neg.list = words[id[myp[id]<0]]
pos.list
pos.list[0:50]
neg.list[0:50]
library(glmnet)
set.seed(2064)
# Use important words
new_dtm_train = dtm_train[,words[id]]
# Logistic Regression
tmpfit = glmnet(x = new_dtm_train,
y = train$sentiment,
alpha = 1,
family='binomial')
voc = read.table("myvocab.txt", stringsAsFactors = FALSE,
header = TRUE)
j = 1
setwd(paste("split_", j, sep=""))
trial_data <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
vectorizer = vocab_vectorizer(create_vocabulary(voc[[1]], ngram = c(1L, 2L)))
it_trial = itoken(trial_data$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_trial = create_dtm(it_trial, vectorizer)
# Logistic Regression
newfit = cv.glmnet(x = dtm_trial,
y = trial_data$sentiment,
alpha = 0.1,
type.measure = 'auc',
family='binomial')
max(newfit$cvm)
# Get test data
setwd(paste("split_", j, sep=""))
test = read.table("test.tsv",
stringsAsFactors = FALSE,
header = TRUE)
test.y = read.table("test_y.tsv",
stringsAsFactors = FALSE,
header = TRUE)
# Regex operation to match html tags in a non-greedy manner
test$review = gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test  = create_dtm(it_test, vectorizer)
preds = predict(newfit, dtm_test, s=newfit$lambda.min)
glmnet:::auc(test.y$sentiment, preds)
preds
mean(preds)
summary(preds)
which(preds > 0.5)
which(preds < 0.5)
length(preds)
templist = list(rep(0,length(preds)))
templist
templist[which(preds > 0.5)] = 1
templist
templist = list(rep(0,length(preds)))
templist
which(preds > 0.5)
templist[which(preds > 0.5)]
templist[which(preds > 0.5),]
templist[,which(preds > 0.5)]
preds
templist[1]
templist[[1]]
templist[0:50]
templist[[0:50]]
templist[[1:50]]
templist[1:5]
typeof(preds)
preds(templist)
typeof(templist)
templist[[4]]
templist[[1]]
templist[[1]][1]
templist[[1]][which(preds > 0.5)]
templist[[1]][which(preds > 0.5)] = 1
templist
test.y$sentiment
templist[1:20]
templist[[1]][1:20]
test.y$sentiment[1:20]
test.y$sentiment[1:50]
templist[[1]][1:50]
templist[[1]][40:100]
test.y$sentiment[40:100]
test.y$sentiment[40:46]
templist[[1]][40:46]
test$review[40]
test$review[46]
dtm_test[40,]
which(dtm[test,] != 0)
which(dtm_test[40,] != 0)
dtm_test[40,which(dtm_test[40,] != 0)]
FN = dtm_test[40,which(dtm_test[40,] != 0)]
sort(FN)
"who" in pos.list
"who" %in% pos.list
"who" %in% neg.list
"money" %in% neg.list
"word" %in% neg.list
"world" %in% neg.list
"will" %in% neg.list
"series" %in% neg.list
"out" %in% neg.list
"off" %in% neg.list
"who" %in% neg.list
"2" %in% neg.list
"out" %in% neg.list
"off" %in% neg.list
"money" %in% neg.list
FP = dtm_test[46,which(dtm_test[46,] != 0)]
sort(FP)
length(voc)
voc
