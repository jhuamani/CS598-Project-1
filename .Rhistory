data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Type"] = "No_Garage"
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Area"] = 0
# Combine bathrooms drop other bathrooms
data$Total_Baths = data$Bsmt_Full_Bath + data$Full_Bath + 0.5 * (data$Bsmt_Half_Bath + data$Half_Bath)
data = subset(data, select=-c(Bsmt_Full_Bath, Full_Bath, Bsmt_Half_Bath, Half_Bath))
# Since Garage Cars and Garage Area are highly correlated, drop Garage Cars
data = subset(data, select=-c(Garage_Cars))
# Check if house has been remodeled
data$Remodeled = 0
data$Remodeled[which((data$Year_Remod_Add - data$Year_Built) > 0)] = 1
data = subset(data, select=-c(Year_Remod_Add))
# Age of purchase
data$House_Age = data$Year_Sold - data$Year_Built
data = subset(data, select=-c(Year_Sold, Year_Built))
# Total house square footage, excluded Mas_Vnr_Area, Lot_Area and Pool_Area
data$Total_SF = data$Total_Bsmt_SF + data$First_Flr_SF + data$Second_Flr_SF + data$Low_Qual_Fin_SF + data$Gr_Liv_Area + data$Garage_Area + data$Wood_Deck_SF + data$Open_Porch_SF + data$Enclosed_Porch + data$Three_season_porch + data$Screen_Porch
data = subset(data, select=-c(Total_Bsmt_SF, BsmtFin_SF_1, BsmtFin_SF_2, Bsmt_Unf_SF, First_Flr_SF, Second_Flr_SF, Low_Qual_Fin_SF, Gr_Liv_Area, Garage_Area, Wood_Deck_SF, Open_Porch_SF, Enclosed_Porch, Three_season_porch, Screen_Porch))
# Hot encode the data
data_mod = dummyVars(~ .,data=data, fullRank = forLinear)
data = data.frame(predict(data_mod, newdata = data))
data
}
# Cleaned and encoded trainData
trainData = clean_data(trainData, forLinear = FALSE)
# Cleaned and encoded testData
testData = clean_data(testData, forLinear = FALSE)
print(dim(trainData))
print(dim(testData))
names_intest_intrain = names(testData)[which((names(testData) %in% names(trainData)))]
names_intrain_nottest = names(trainData)[which(!(names(trainData) %in% names(testData)))]
# Remove any predictors in testData not present in trainData
if (length(names_intest_intrain) != 0) {
testData = testData[,names_intest_intrain]
}
# For any predictor present in trainData but not testData add 0 columns
if (length(names_intrain_nottest) != 0) {
for (name in names_intrain_nottest) {
if (name != "Sale_Price"){
testData[,name] = 0
}
}
}
# Final check to ensure that only column that is different is Sale_Price
print(names(testData)[which(!(names(testData) %in% names(trainData)))])
print(names(trainData)[which(!(names(trainData) %in% names(testData)))])
train_mat_df = subset(trainData, select=-c(Sale_Price))
train_mat = as.matrix(train_mat_df[,order(colnames(train_mat_df))])
train_Y = trainData$Sale_Price
test_mat = as.matrix(testData[,order(colnames(testData))])
print(dim(train_mat))
print(dim(test_mat))
# Original eta = 0.05 and nrounds = 5000
xgb.model = xgboost(data = train_mat,
label = train_Y, max_depth = 6,
eta = 0.04, nrounds = 6000,
subsample = 0.5,
verbose = FALSE)
summary(xgb.model)
tmp = predict(xgb.model, test_mat)
sqrt(mean((log(tmp) - log(test.y[,2]))^2))
stop.time = Sys.time()
print(start.time - stop.time)
start.time = Sys.time()
library(caret)
library(xgboost)
set.seed(2064)
start.time = Sys.time()
library(caret)
library(xgboost)
set.seed(2064)
myData = read.csv("Ames_data.csv")
testIDs = read.table("project1_testIDs.dat")
j = 5
train = myData[-testIDs[,j], ]
test = myData[testIDs[,j], ]
test.y = test[, c(1, 83)]
test = test[, -83]
write.csv(train,"train.csv",row.names=FALSE)
write.csv(test, "test.csv",row.names=FALSE)
write.csv(test.y,"test_y.csv",row.names=FALSE)
trainData = read.csv("train.csv")
testData = read.csv("test.csv")
test.y = read.csv("test_y.csv")
i = 0
start = 0
stop = 50
for (name in names(trainData[,sapply(trainData,class) == "character"])) {
if (i >= start){
cat("\nPredictor ",name,"\n\n")
print(unique(trainData[,name]))
}
i = i+1
if (i == stop) {break}
}
# Here we check if any missing values exist in any predictor
NAcols = apply(trainData,2, function(x) any(is.na(x)))
print(which(NAcols==TRUE))
clean_data = function(data, forLinear = TRUE){
# Electrical Column: Replace Unknown with highest frequency SBrkr
data[which(data$Electrical == "Unknown"),"Electrical"] = "SBrkr"
# Outlier in Garage_Yr_Blt
data[which(data$Garage_Yr_Blt == 2207),"Garage_Yr_Blt"] = 2006
data[,"Garage_Yr_Blt"] = as.character(data[,"Garage_Yr_Blt"])
data[is.na(data[,"Garage_Yr_Blt"]),"Garage_Yr_Blt"] = "No_Garage"
# After revieweing correlations, Garage_Yr_Blt should be removed as it adds too many excess columns with not much added value
data = subset(data, select=-c(Garage_Yr_Blt))
# Outlier constraint to 95% quantile
num.vars = c("Lot_Frontage", "Lot_Area", "Mas_Vnr_Area", "BsmtFin_SF_2", "Bsmt_Unf_SF", "Total_Bsmt_SF", "Second_Flr_SF", 'First_Flr_SF', "Gr_Liv_Area", "Garage_Area", "Wood_Deck_SF", "Open_Porch_SF", "Enclosed_Porch", "Three_season_porch", "Screen_Porch", "Misc_Val")
quan.val = 0.95
for (var in num.vars) {
temp = data[,var]
quan = quantile(temp, probs = quan.val, na.rm = TRUE)
temp[temp > quan] = quan
data[,var] = temp
}
# Remove unnecessary columns
data = subset(data, select=-c(Street, Utilities, Condition_2, Roof_Matl, Heating, Pool_QC, Misc_Feature, Pool_Area, Longitude,Latitude))
# Outputs are not the same, some houses with No_Garages have a Garage_Type filled in and Garage_Cars > 0. This needs to be corrected
# A single entry has a Garage_Area of 360, we'll assume that it's 0
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Cars"] = 0
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Type"] = "No_Garage"
data[which(data[,"Garage_Cond"] == "No_Garage"),"Garage_Area"] = 0
# Combine bathrooms drop other bathrooms
data$Total_Baths = data$Bsmt_Full_Bath + data$Full_Bath + 0.5 * (data$Bsmt_Half_Bath + data$Half_Bath)
data = subset(data, select=-c(Bsmt_Full_Bath, Full_Bath, Bsmt_Half_Bath, Half_Bath))
# Since Garage Cars and Garage Area are highly correlated, drop Garage Cars
data = subset(data, select=-c(Garage_Cars))
# Check if house has been remodeled
data$Remodeled = 0
data$Remodeled[which((data$Year_Remod_Add - data$Year_Built) > 0)] = 1
data = subset(data, select=-c(Year_Remod_Add))
# Age of purchase
data$House_Age = data$Year_Sold - data$Year_Built
data = subset(data, select=-c(Year_Sold, Year_Built))
# Total house square footage, excluded Mas_Vnr_Area, Lot_Area and Pool_Area
data$Total_SF = data$Total_Bsmt_SF + data$First_Flr_SF + data$Second_Flr_SF + data$Low_Qual_Fin_SF + data$Gr_Liv_Area + data$Garage_Area + data$Wood_Deck_SF + data$Open_Porch_SF + data$Enclosed_Porch + data$Three_season_porch + data$Screen_Porch
data = subset(data, select=-c(Total_Bsmt_SF, BsmtFin_SF_1, BsmtFin_SF_2, Bsmt_Unf_SF, First_Flr_SF, Second_Flr_SF, Low_Qual_Fin_SF, Gr_Liv_Area, Garage_Area, Wood_Deck_SF, Open_Porch_SF, Enclosed_Porch, Three_season_porch, Screen_Porch))
# Hot encode the data
data_mod = dummyVars(~ .,data=data, fullRank = forLinear)
data = data.frame(predict(data_mod, newdata = data))
data
}
# Cleaned and encoded trainData
trainData = clean_data(trainData, forLinear = FALSE)
# Cleaned and encoded testData
testData = clean_data(testData, forLinear = FALSE)
print(dim(trainData))
print(dim(testData))
names_intest_intrain = names(testData)[which((names(testData) %in% names(trainData)))]
names_intrain_nottest = names(trainData)[which(!(names(trainData) %in% names(testData)))]
# Remove any predictors in testData not present in trainData
if (length(names_intest_intrain) != 0) {
testData = testData[,names_intest_intrain]
}
# For any predictor present in trainData but not testData add 0 columns
if (length(names_intrain_nottest) != 0) {
for (name in names_intrain_nottest) {
if (name != "Sale_Price"){
testData[,name] = 0
}
}
}
# Final check to ensure that only column that is different is Sale_Price
print(names(testData)[which(!(names(testData) %in% names(trainData)))])
print(names(trainData)[which(!(names(trainData) %in% names(testData)))])
train_mat_df = subset(trainData, select=-c(Sale_Price))
train_mat = as.matrix(train_mat_df[,order(colnames(train_mat_df))])
train_Y = trainData$Sale_Price
test_mat = as.matrix(testData[,order(colnames(testData))])
print(dim(train_mat))
print(dim(test_mat))
# Original eta = 0.05 and nrounds = 5000
xgb.model = xgboost(data = train_mat,
label = train_Y, max_depth = 6,
eta = 0.04, nrounds = 6000,
subsample = 0.5,
verbose = FALSE)
summary(xgb.model)
tmp = predict(xgb.model, test_mat)
sqrt(mean((log(tmp) - log(test.y[,2]))^2))
stop.time = Sys.time()
print(start.time - stop.time)
library(MASS)
attach(Boston)
library(MASS)
attach(Boston)
dis
q8 = poly(nox ~ dis, degree = 3)
head(Boston)
#q8 = lm(nox ~ poly(dis,3), )
#head(Boston)
q8 = lm(nox ~ poly(dis,3), Boston)
#head(Boston)
q8 = lm(nox ~ poly(dis,3), Boston)
summary(q8)
q8$residuals
sum((q8$residuals)^2)
predict(q8, newdata=list(dis=6))
q11 = lm(nox ~ poly(dis,4), Boston)
summary(q11)
sum((q11$residuals)^2)
predict(q11, newdata=list(dis=6))
myfit1 = lm(nox ~ bs(dis, df=3), data=Boston)
library(splines)
myfit1 = lm(nox ~ bs(dis, df=3), data=Boston)
myfit2 = lm(nox ~ bs(dis, df=4, intercept=TRUE), data=Boston)
myfit3 = lm(nox ~ poly(dis,3), data=Boston)
print(sum((myfit1$residuals)^2))
print(sum((myfit2$residuals)^2))
print(sum((myfit3$residuals)^2))
library(splines)
myfit1 = lm(nox ~ bs(dis, df=4), data=Boston)
myfit2 = lm(nox ~ bs(dis, df=5, intercept=TRUE), data=Boston)
myfit3 = lm(nox ~ bs(dis,knots=median(dis)), data=Boston)
print(sum((myfit1$residuals)^2))
print(sum((myfit2$residuals)^2))
print(sum((myfit3$residuals)^2))
Estep <- function(data, G, para){
# Your Code
# Return the n-by-G probability matrix
n = dim(data)[1]
z = outer(1:n, 1:G,FUN = Vectorize(function(i, j)
-(1/2) * as.matrix(data[i,] - para$mean[,j]) %*% solve(para$Sigma) %*% t(as.matrix(data[i,] - para$mean[,j])) + log(para$prob[j]) )
)
# Subtracting max value of z to handle underflow problem
z_max = apply(z, 1, max)
z = exp(z - z_max) / rowSums(exp(z - z_max))
z
}
Mstep <- function(data, G, para, post.prob){
# Your Code
# Return the updated parameters
n = dim(data)[1]
d = dim(data)[2]
para$prob = colSums(post.prob) / n
para$mean = outer(1:d, 1:G,FUN = Vectorize(function(i, j)
sum(as.matrix(post.prob[,j]*data[,i])) / sum(as.matrix(post.prob[,j]))
) )
para$Sigma = matrix(0,d,d)
for (g in 1:G){
for (i in 1:n){
para$Sigma = para$Sigma + (t(data[i,]) - para$mean[,g]) %*% t(t(data[i,]) - para$mean[,g]) * post.prob[i,g]
}
}
para$Sigma = para$Sigma / n
para
}
myEM <- function(data, itmax, G, para){
# itmax: num of iterations
# G:     num of components
# para:  list of parameters (prob, mean, Sigma)
for(t in 1:itmax){
post.prob <- Estep(data, G, para)
para <- Mstep(data, G, para, post.prob)
}
return(para)
}
options(digits=8)
options()$digits
library(mclust)
dim(faithful)
head(faithful)
n = nrow(faithful)
K <- 2
set.seed(2064)
gID <- sample(1:K, n, replace = TRUE)
Z <- matrix(0, n, K)
for(k in 1:K)
Z[gID == k, k] <- 1
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
para0 <- list(prob = ini0$pro,
mean = ini0$mean,
Sigma = ini0$variance$Sigma)
para0
myEM(data=faithful, itmax=20, G=K, para=para0)
Rout <- em(modelName = "EEE", data = faithful,
control = emControl(eps=0, tol=0, itmax = 20),
parameters = ini0)$parameters
list(Rout$pro, Rout$mean, Rout$variance$Sigma)
K <- 3
gID <- sample(1:K, n, replace = TRUE)
Z <- matrix(0, n, K)
for(k in 1:K)
Z[gID == k, k] <- 1
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
para0 <- list(prob = ini0$pro,
mean = ini0$mean,
Sigma = ini0$variance$Sigma)
para0
myEM(data=faithful, itmax=20, G=K, para=para0)
Rout <- em(modelName = "EEE", data = faithful,
control = emControl(eps=0, tol=0, itmax = 20),
parameters = ini0)$parameters
list(Rout$pro, Rout$mean, Rout$variance$Sigma)
setwd("~/GitHub/CS598-Project-1")
Estep <- function(data, G, para){
# Your Code
# Return the n-by-G probability matrix
n = dim(data)[1]
z = outer(1:n, 1:G,FUN = Vectorize(function(i, j)
-(1/2) * as.matrix(data[i,] - para$mean[,j]) %*% solve(para$Sigma) %*% t(as.matrix(data[i,] - para$mean[,j])) + log(para$prob[j]) )
)
# Subtracting max value of z to handle underflow problem
z_max = apply(z, 1, max)
z = exp(z - z_max) / rowSums(exp(z - z_max))
z
}
Mstep <- function(data, G, para, post.prob){
# Your Code
# Return the updated parameters
n = dim(data)[1]
d = dim(data)[2]
para$prob = colSums(post.prob) / n
para$mean = outer(1:d, 1:G,FUN = Vectorize(function(i, j)
sum(as.matrix(post.prob[,j]*data[,i])) / sum(as.matrix(post.prob[,j]))
) )
para$Sigma = matrix(0,d,d)
for (g in 1:G){
for (i in 1:n){
para$Sigma = para$Sigma + (t(data[i,]) - para$mean[,g]) %*% t(t(data[i,]) - para$mean[,g]) * post.prob[i,g]
}
}
para$Sigma = para$Sigma / n
para
}
myEM <- function(data, itmax, G, para){
# itmax: num of iterations
# G:     num of components
# para:  list of parameters (prob, mean, Sigma)
for(t in 1:itmax){
post.prob <- Estep(data, G, para)
para <- Mstep(data, G, para, post.prob)
}
return(para)
}
options(digits=8)
options()$digits
library(mclust)
dim(faithful)
head(faithful)
n = nrow(faithful)
K <- 2
set.seed(2064)
gID <- sample(1:K, n, replace = TRUE)
Z <- matrix(0, n, K)
for(k in 1:K)
Z[gID == k, k] <- 1
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
para0 <- list(prob = ini0$pro,
mean = ini0$mean,
Sigma = ini0$variance$Sigma)
para0
myEM(data=faithful, itmax=20, G=K, para=para0)
Rout <- em(modelName = "EEE", data = faithful,
control = emControl(eps=0, tol=0, itmax = 20),
parameters = ini0)$parameters
list(Rout$pro, Rout$mean, Rout$variance$Sigma)
K <- 3
gID <- sample(1:K, n, replace = TRUE)
Z <- matrix(0, n, K)
for(k in 1:K)
Z[gID == k, k] <- 1
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
para0 <- list(prob = ini0$pro,
mean = ini0$mean,
Sigma = ini0$variance$Sigma)
para0
myEM(data=faithful, itmax=20, G=K, para=para0)
Rout <- em(modelName = "EEE", data = faithful,
control = emControl(eps=0, tol=0, itmax = 20),
parameters = ini0)$parameters
list(Rout$pro, Rout$mean, Rout$variance$Sigma)
myBW = function(x, para, n.iter = 100){
# Input:
# x: T-by-1 observation sequence
# para: initial parameter value
# Output updated para value (A and B; we do not update w)
for(i in 1:n.iter){
para = BW.onestep(x, para)
}
return(para)
}
BW.onestep = function(x, para){
T = length(x)
mz = para$mz
mx = para$mx
A = para$A
B = para$B
w = para$w
alp = forward.prob(x, para)
beta = backward.prob(x, para)
myGamma = array(0, dim=c(mz, mz, T-1))
## YOUR CODE:
## Compute gamma_t(i,j) P(Z[t] = i, Z[t+1]=j),
## for t=1:T-1, i=1:mz, j=1:mz,
## which are stored an array, myGamma
for (t in 1:(T-1)){
for(i in 1:mz)
for(j in 1:mz)
myGamma[i,j,t]= alp[t,i]*A[i,j]* B[j, x[t+1]] *beta[t+1,j]
}
# for (t in 1:(T-1)){
# myGamma[,,t]= outer(alp[t, ], B[, x[t+1]] * beta[t+1,]) * A
#}
# M-step for parameter A
A = rowSums(myGamma, dims = 2)
A = A/rowSums(A)
# M-step for parameter B
tmp = apply(myGamma, c(1, 3), sum)  # mz-by-(T-1)
tmp = cbind(tmp, colSums(myGamma[, , T-1]))
for(l in 1:mx){
B[, l] = rowSums(tmp[, which(x==l)])
}
B = B/rowSums(B)
para$A = A
para$B = B
return(para)
}
forward.prob = function(x, para){
# Output the forward probability matrix alp
# alp: T by mz, (t, i) entry = P(x_{1:t}, Z_t = i)
T = length(x)
mz = para$mz
A = para$A
B = para$B
w = para$w
alp = matrix(0, T, mz)
# fill in the first row of alp
alp[1, ] = w * B[, x[1]]
# Recursively compute the remaining rows of alp
for(t in 2:T){
tmp = alp[t-1, ] %*% A
alp[t, ] = tmp * B[, x[t]]
}
return(alp)
}
backward.prob = function(x, para){
# Output the backward probability matrix beta
# beta: T by mz, (t, i) entry = P(x_{1:t}, Z_t = i)
T = length(x)
mz = para$mz
A = para$A
B = para$B
w = para$w
beta = matrix(1, T, mz)
# The last row of beta is all 1.
# Recursively compute the previous rows of beta
for(t in (T-1):1){
tmp = as.matrix(beta[t+1, ] * B[, x[t+1]])  # make tmp a column vector
beta[t, ] = t(A %*% tmp)
}
return(beta)
}
myViterbi = function(x, para){
# Output: most likely sequence of Z (T-by-1)
T = length(x)
mz = para$mz
A = para$A
B = para$B
w = para$w
log.A = log(A)
log.w = log(w)
log.B = log(B)
# Compute delta (in log-scale)
delta = matrix(0, T, mz)
# fill in the first row of delta
delta[1, ] = log.w + log.B[, x[1]]
## YOUR CODE:
for (t in 2:T){
for (i in 1:mz){
delta[t,i] = max(delta[t-1,]+log.A[,i])+log.B[i, x[t]]
}
}
## Recursively compute the remaining rows of delta
# Compute most prob sequence Z
Z = rep(0, T)
# start with the last entry of Z
Z[T] = which.max(delta[T, ])
## YOUR CODE:
Z[T] = which.max(delta[T,])
for (t in (T-1):1){
Z[t] = which.max(delta[t,]+log.A[,Z[t+1]])
}
## Recursively compute the remaining entries of Z
return(Z)
}
data = scan("coding4_part2_data.txt")
mz = 2
mx = 3
ini.w = rep(1, mz); ini.w = ini.w / sum(ini.w)
ini.A = matrix(1, 2, 2); ini.A = ini.A / rowSums(ini.A)
ini.B = matrix(1:6, 2, 3); ini.B = ini.B / rowSums(ini.B)
ini.para = list(mz = 2, mx = 3, w = ini.w,A = ini.A, B = ini.B)
myout = myBW(data, ini.para, n.iter = 100)
myout.Z = myViterbi(data, myout)
myout.Z[myout.Z==1] = 'A'
myout.Z[myout.Z==2] = 'B'
library(HMM)
hmm0 =initHMM(c("A", "B"), c(1, 2, 3),
startProbs = ini.w,
transProbs = ini.A,
emissionProbs = ini.B)
Rout = baumWelch(hmm0, data, maxIterations=100, delta=1E-9, pseudoCount=0)
Rout.Z = viterbi(Rout$hmm, data)
options(digits=8)
options()$digits
myout$A
Rout$hmm$transProbs
myout$B
Rout$hmm$emissionProbs
cbind(Rout.Z, myout.Z)[c(1:10, 180:200), ]
sum(Rout.Z != myout.Z)
